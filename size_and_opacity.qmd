---
format: acm-pdf

# use keep-tex to cause quarto to generate a .tex file
# which you can eventually use with TAPS
keep-tex: true

bibliography: size-opacity.bib

params:
  eval_models: false
  
knitr:
  opts_chunk: 
    cache_comments: false
    crop: true
    
execute: 
  echo: false
  warning: false
  message: false
  include: false

title: Effects of Point Size and Opacity Adjustments in Scatterplots

# if short-title is defined, then it's used
short-title: Size, Opacity, and Scatterplots

author:
  - name: Gabriel Strain
    email: gabriel.strain@manchester.ac.uk
    orcid: 0000-0002-4769-9221
    affiliation:
      name: Department of Computer Science, Faculty of Science and Engineering, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL
  - name: Andrew J. Stewart
    email: andrew.j.stewart@manchester.ac.uk
    affiliation:
      name: Department of Computer Science, Faculty of Science and Engineering, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL
  - name: Paul Warren
    email: paul.warren@manchester.ac.uk
    affiliation:
      name: Division of Psychology, Communication and Human Neuroscience, School of Health Sciences, Faculty of Biology, Medicine, and Health, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL
  - name: Caroline Jay
    affiliation:
      name: Department of Computer Science, Faculty of Science and Engineering, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL

# acm-specific metadata
acm-metadata:
  # comment this out to make submission anonymous
  anonymous: false

  # comment this out to build a draft version
  final: true

  # comment this out to specify detailed document options
  acmart-options: sigconf

  # acm preamble information
  copyright-year: 2024
  acm-year: 2024
  copyright: rightsretained
  doi: 10.1145/3613904.3642127
  conference-acronym: "CHI'24"
  conference-name: |
    the CHI Conference on Human Factors in Computing Systems
  conference-date: May 11--16, 2024
  conference-location: Honolulu, HI, USA
  isbn: 979-8-4007-0330-0/24/05

  # if present, replaces the list of authors in the page header.
  shortauthors: Strain et al.

  # The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
  # Please copy and paste the code instead of the example below.
  ccs: |
    \begin{CCSXML}
    <ccs2012>
      <concept>
        <concept_id>10003120.10003145.10011770</concept_id>
        <concept_desc>Human-centered computing~Visualization design and evaluation methods</concept_desc>
        <concept_significance>300</concept_significance>
        </concept>
      <concept>
       <concept_id>10003120.10003121.10011748</concept_id>
       <concept_desc>Human-centered computing~Empirical studies in HCI</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
      <concept>
       <concept_id>10003120.10003121.10003122</concept_id>
       <concept_desc>Human-centered computing~HCI design and evaluation methods</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
      <concept>
       <concept_id>10003120.10003145.10011769</concept_id>
       <concept_desc>Human-centered computing~Empirical studies in visualization</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
    </ccs2012>
    \end{CCSXML}

    \ccsdesc[300]{Human-centered computing~Visualization design and evaluation methods}
    \ccsdesc[500]{Human-centered computing~Empirical studies in HCI}
    \ccsdesc[300]{Human-centered computing~HCI design and evaluation methods}
    \ccsdesc[500]{Human-centered computing~Empirical studies in visualization}

  keywords:
    - correlation
    - scatterplot
    - perception
    - crowdsourced

abstract: |
  Systematically changing the size and opacity of points on scatterplots can be used to induce more accurate perceptions of correlation by viewers. Evidence points to the mechanisms behind these effects being similar, so one may expect their combination to be additive regarding their effects on correlation estimation. We present a fully-reproducible study in which we combine techniques for influencing correlation perception to show that in reality, effects of changing point size and opacity interact in a non-additive 
  fashion. We show that there is a great deal of scope for using visual features to change viewers' perceptions of data visualizations. Additionally, we use our results to further interrogate the perceptual mechanisms at play when changing point size and opacity in scatterplots.
  
---
```{r}
#| label: setup

set.seed(1234) # seed for all random number generation

# Loading packages

library(tidyverse)
library(MASS)
library(emmeans)
library(scales)
library(buildmer)
library(lme4)
library(kableExtra)
library(papaja)
library(qwraps2)
library(lmerTest)
library(ggdist)
library(ggpubr)
library(conflicted)
library(ggtext)
library(r2glmm)
library(grid)
library(DescTools)
library(Matrix)

# fix conflicts now using the conflicted package

conflicts_prefer(dplyr::select(), dplyr::filter(), lme4::lmer(), DescTools::AUC())

# create labels vector for use with plotting functions

labels_size_opacity <- c(A = "Typical Orientation Size\nTypical Orientation Opacity",
                          B = "Inverted Orientation Size\nInverted Orientation Opacity",
                          X = "Typical Orientation Size\nInverted Orientation Opacity",
                          Y = "Inverted Orientation Size\nTypical Orientation Opacity")

labels_all_exp <- c(additive_manipulation = "Size and Opacity Manipulated",
                    opacity_manipulated = "Opacity Manipulated",
                    size_manipulated = "Size Manipulated",
                    standard_plot = "No Manipulation Present")

labels_power <- c(size_power = "Size\nPower",
                  opacity_power = "Opacity\nPower",
                  additive_power = "Size and Opacity\nCombined Power")
```

```{r}
#| label: lazyload-cache

if (!params$eval_models){ lazyload_cache_dir("size_and_opacity_cache/pdf") }
```

```{r}
#| label: load-data

# load in data file
# Need higher guess_max so that read_csv() guesses column types correctly 

additive_anon <- read_csv("data/additive_data.csv", guess_max = 17000)
```

```{r}
#| label: wrangle-data

## NB: With the exception of anonymization, data are provided as-is from 
## pavlovia (survey tool). Wrangling function *must* be run first to make
## the data set usable

# first do literacy

wrangle <- function(anon_file) {
  
  literacy <- anon_file %>%
    filter(!is.na(q5_slider.response)) %>%
    rowwise() %>%
    mutate(literacy = sum(c(q1_slider.response, 
                            q2_slider.response, 
                            q3_slider.response, 
                            q4_slider.response, 
                            q5_slider.response))) %>%
    select(participant,
           literacy)
  
# extract and process visual threshold testing
  
visual_thresholds <- anon_file %>%
    filter(!is.na(VT_with_labels)) %>%
    select(c("VT_with_labels",
             "participant",
             "VT_textbox2.text")) %>%
    mutate(VT_answer = str_replace(VT_with_labels,
                                   pattern = "vis_threshold_plots/",
                                   replacement = "")) %>%
    mutate(VT_answer = str_replace(VT_answer,
                                   pattern = "_VT.png",
                                   replacement = "")) %>%
    mutate(correct_VT = case_when(
      VT_answer == VT_textbox2.text ~ "y",
      VT_answer != VT_textbox2.text ~ "n",
      is.na(VT_answer) ~ "n", TRUE ~ as.character(VT_answer))) %>%
    group_by(participant) %>% 
    summarise(VT_no_correct = sum(correct_VT == "y")) %>%
    mutate(VT_perc_correct = (VT_no_correct/6)*100) %>%
    select("VT_perc_correct",
           "VT_no_correct",
           "participant")
  
# extract and process monitor and dot pitch information
# we assume standard 16:9 aspect ratio for monitors
  
monitor_information <- anon_file %>%
  mutate(height = dplyr::lead(height)) %>%
  mutate(res_height = res_width*0.5625,
         width = height*1.777,
         dot_pitch = ((sqrt(height^2 + width^2))/(sqrt(res_height^2 + res_width^2))) * 25.4) %>%
         select(c("dot_pitch",
                  "participant",
                  "res_width",
                  "width",
                  "height")) %>%
  na.omit()
  
# extract demographic information
# link slider response numbers to gender categories
  
demographics <- anon_file %>%
    filter(!is.na(gender_slider.response)) %>%
    mutate(gender_slider.response = recode(gender_slider.response,
                                         `1` = "F",
                                         `2` = "M",
                                         `3` = "NB")) %>%
  select(matches(c("participant",
                    "age_textbox.text",
                    "gender_slider.response")))

# split images column into item and condition columns
# additionally create "condition_abs" column
# this will be simpler to plot with later

anon_file <- anon_file %>%
  mutate(images = str_replace(images, pattern = "A", replacement = "-S-S")) %>%
  mutate(images = str_replace(images, pattern = "B", replacement = "-I-I")) %>%
  mutate(images = str_replace(images, pattern = "X", replacement = "-S-I")) %>%
  mutate(images = str_replace(images, pattern = "Y", replacement = "-I-S")) %>%
  separate(images, c("item",
                     "opacity",
                     "size"),
           sep = "-") %>%
  mutate(size = str_replace(size, pattern = ".png", replacement = "")) %>%
  mutate(condition_abs = case_when(
    opacity == "S" & size == "S" ~ "A",
    opacity == "I" & size == "I" ~ "B",
    opacity == "S" & size == "I" ~ "X",
    opacity == "I" & size == "S" ~ "Y",
    TRUE ~ "placeholder"
  ))

# select relevant columns
# select only experimental items
# add literacy data
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  select(c("participant",
            "item",
            "size",
            "opacity",
            "slider.response",
            "my_rs",
            "total_residuals",
            "unique_item_no",
            "session",
            "trials.thisN",
            "condition_abs")) %>%
  mutate(half = case_when(
    trials.thisN < 93 ~ "First",
    trials.thisN > 92 ~ "Second" )) %>% # used for training testing later on
  filter(unique_item_no < 181) %>%
  inner_join(literacy, by = "participant") %>%
  inner_join(demographics, by = "participant") %>%
  inner_join(monitor_information, by = "participant") %>%
  inner_join(visual_thresholds, by = "participant") %>%
  mutate(across(matches(c("item", "opacity", "size", "condition_abs")), as_factor)) %>%
  mutate(trials.thisN = as.integer(trials.thisN)) %>%
  mutate(difference = my_rs - slider.response) %>%
  select(-c("__participant")) %>%
  assign(paste0(unique(anon_file$expName), "_tidy"),
           value = ., envir = .GlobalEnv)
}

# use wrangle function on anonmyised data file 

wrangle(additive_anon)

# set deviation coding for experimental model

contrasts(size_and_opacity_exp_tidy$size) <- matrix(c(.5, -.5))
contrasts(size_and_opacity_exp_tidy$opacity) <- matrix(c(.5, -.5))

# remove anon df from environment

rm(additive_anon)

# extract age data

age <- distinct(size_and_opacity_exp_tidy, participant,
                .keep_all = TRUE) %>%
  summarise(mean = mean(age_textbox.text, na.rm = TRUE),
            sd = sd(age_textbox.text, na.rm = TRUE)) 

  sum(is.na(size_and_opacity_exp_tidy$age_textbox.text))
  
# extract gender data

gender <- distinct(size_and_opacity_exp_tidy, participant,
                      .keep_all = TRUE) %>%
  group_by(gender_slider.response) %>%
  summarise(perc = n()/nrow(.)*100) %>%
  pivot_wider(names_from = gender_slider.response, values_from = perc)

# extract literacy data

literacy <- distinct(size_and_opacity_exp_tidy, participant,
                        .keep_all = TRUE) %>%
  summarise(mean = mean(literacy), sd = sd(literacy))

# extract visual threshold data

VT <- size_and_opacity_exp_tidy %>%
  summarise(mean_VT_no_correct = mean(VT_no_correct),
            sd_VT_no_correct = sd(VT_no_correct),
            mean_VT_perc_correct = mean(VT_perc_correct),
            sd_VT_perc_correct = sd(VT_perc_correct))

# extract dot pitch data

dot_pitch <- size_and_opacity_exp_tidy %>%
  summarise(mean_dp = mean(dot_pitch),
            sd_dp = sd(dot_pitch),
            mean_width = median(width),
            mean_height = median(height))
```

```{r}
#| label: comparison-function

# this function takes a model and creates a nested model with the fixed effects 
# terms removed for anova comparison

comparison <- function(model) {
  
  parens <- function(x) paste0("(",x,")")
  onlyBars <- function(form) reformulate(sapply(findbars(form),
                                              function(x)  parens(deparse(x))),
                                       response=".")
  onlyBars(formula(model))
  cmpr_model <- update(model,onlyBars(formula(model)))
  
  return(cmpr_model)
  
}
```

```{r}
#| label: anova-results-function

# this function takes two nested models, runs an anova, and the outputs the 
# Chi-square statistic, the degrees of freedom, and the p value to the global 
# environment

anova_results <- function(model, cmpr_model) {
  
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  if (class(cmpr_model) == "buildmer") cmpr_model <- cmpr_model@model
  
  anova_output <- anova(model, cmpr_model)
  
  assign(paste0(model_name, ".Chisq"),
         anova_output$Chisq[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".df"),
         anova_output$Df[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".p"),
         anova_output$`Pr(>Chisq)`[2],
         envir = .GlobalEnv)
  
}
```

```{r}
#| label: contrasts-extract

# this function extracts test statistics and p values from model summaries

contrasts_extract <- function(model) {
  
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  
  EMMs <- emmeans(model, pairwise ~ size * opacity)
  
  contrast_df <- as.data.frame(EMMs[2]) %>%
                            rename_with(str_replace,
                                        pattern = "contrasts.", replacement = "",
                                        matches("contrasts")) %>%
                            rename_with(str_to_title, !starts_with("p")) %>%
                            select(c("Contrast", "Z.ratio", "p.value"))
  
  return(contrast_df)
}
```

```{r}
#| label: error-bar-plot

# plot the error bars plots by condition
# takes dataframe, measure (i.e difference or raw r score), and label vector

plot_error_bars_function <- function(df, measure, l){
  df %>% 
  drop_na() %>% 
  group_by(condition_abs, my_rs) %>% 
  summarise(sd = sd(get(measure)), mean = mean(get(measure))) %>% 
  ggplot(aes(x = my_rs, y = mean*-1)) +
  geom_point(size = 0.2) + 
  geom_errorbar(mapping = aes(ymin = -1*mean + sd, ymax = -1*mean - sd),
                width = 0.01,
                size = 0.3) +
  theme_ggdist() +
  scale_y_continuous(breaks = seq(-0.4,1, 0.2)) +
  theme(strip.text = element_text(size = 6,
                                  margin = margin(1,0,1,0, "mm")),
        aspect.ratio = 1,
        axis.text = element_text(size = 7),
        axis.title.y = ggtext::element_markdown(size = 8),
        axis.title.x = ggtext::element_markdown(size = 8)) +
  facet_wrap(condition_abs ~., ncol = 4, labeller = labeller(condition_abs = l)) +
    labs(x = "Objective *r*",
         y = "Mean *r* Estimation Error") +
    geom_line(formula= x ~ y) +
    xlim(0.2,1)
}
```

```{r}
#| label: make-sig-table-lm

# function to create table with fixed effects and interactions for LMM

make_sig_table <- function (model) {
  
  # buildmer class models need reassigned to lmer class for further use
  
  if (class(model) == "buildmer") model <- model@model
  
  # subset model summary to get list of fixed/interaction effects
  # then make this a data frame, rename columns, fix p value formatting
  
  table_df <- as.data.frame(summary(model)[10]) %>%
    rename("Estimate" = "coefficients.Estimate",
           "Standard Error" = "coefficients.Std..Error",
           "df" = "coefficients.df",
           "t-value" = "coefficients.t.value",
           "p" = "coefficients.Pr...t..") %>%
    mutate(p = scales::pvalue(p)) %>%
    rename("\\textit{p}" = "p")
  
  # get semi-partial r^2, rename rows so they match with sig table
  # take the last 3 rows corresponding to fixed effects and interaction term
  # remove first cell of r^2
  # reformat text for latex output
  
  r_squared <- r2beta(model, method = "nsj") %>%
    mutate("Effect" = recode(Effect,
                             "size" = "Size Decay",
                             "opacity" = "Opacity Decay",
                             "size:opacity" = "Size Decay x Opacity Decay")) %>%
    select(c("Effect", "Rsq")) %>%
    mutate(Rsq = round(Rsq, 3)) %>%
    mutate(Rsq = ifelse(row_number() == 1, "", Rsq)) %>%
    rename("R\\textsuperscript{2}" = "Rsq") 
    
  # tidy up row names
  
  rownames(table_df) <- c("(Intercept)",
                          "Size Decay",
                          "Opacity Decay",
                          "Size Decay x Opacity Decay")
  
  sig_and_squared <- cbind(table_df, r_squared) %>% select(-c("Effect")) 
  
  table <- kbl(sig_and_squared, booktabs = T, digits = c(2,3,2,2,2,2,3), escape = F)
  
  return(table)
}
```

```{r}
#| label: plot-examples-function
  
  set.seed(1234)
  
  my_sample_size = 128
  
  my_desired_r = 0.6
  
  mean_variable_1 = 0
  sd_variable_1 = 1
  
  mean_variable_2 = 0
  sd_variable_2 = 1
  
  mu <- c(mean_variable_1, mean_variable_2) 
  
  myr <- my_desired_r * sqrt(sd_variable_1) * sqrt(sd_variable_2)
  
  mysigma <- matrix(c(sd_variable_1, myr, myr, sd_variable_2), 2, 2) 
  
  corr_data = as_tibble(mvrnorm(my_sample_size, mu, mysigma, empirical = TRUE))
  
  corr_model <- lm(V2 ~ V1, data = corr_data)
  
  my_residuals <- abs(residuals(corr_model))
  
  data_with_resid <- round(cbind(corr_data, my_residuals), 2)
  
slopes <- data_with_resid %>%
  mutate(slope_0.25 = 1-(0.25)^my_residuals) %>%
  mutate(slope_inverted = (1 + (0.25)^ my_residuals)-1) %>%
  mutate(slope_inverted_floored = pmax(0.1,(1+(0.25)^my_residuals)-1)) 
  
plot_example_function <- function (data, size, opacity, title) {
  
set.seed(1234)
  
  ggplot(data, aes(x = V1, y = V2)) +
  scale_size_identity() +
  geom_point(aes(size = 2*(size + 0.1),
                 alpha = opacity),
             shape = 16)  +
  theme_classic() +
  theme(axis.text = element_blank(),
        plot.margin = unit(c(0,0,0.8,0), "cm"),
        legend.position = "none",
        plot.title = element_text(size = 6.5, vjust = -0.1),
        axis.title = element_blank(),
        axis.line = element_line(linewidth = 0.25),
        axis.ticks = element_line(linewidth = 0.25),
        axis.ticks.length = unit(1.375, "pt")) +
  labs(title = title)

} 
```

# Introduction

Scatterplots are used for a wide variety of communicative tasks, both in academic and
non-academic contexts. While most commonly used to represent linear correlation,
or the degree of linear relatedness between two variables,
they can also be used to represent different groups (clustering), to aid in the detection of outliers,
to characterize distributions, and to visualize non-linear correlations.
@fig-tasks contains examples of scatterplots optimized for different tasks. There
is evidence that people generally interpret scatterplots in similar ways [@kay_2015], and
that they support the interpretation of correlation significantly better than competing
data visualizations [@li_2010]. Rapid interpretation by viewers [@rensink_2014], along
with low levels of interindividual variance render scatterplots particularly suited
for experimental work; they provide important insights into perception and visualization design while
being simple to study [@rensink_2014]. 

While our interpretations of scatterplots are generally similar, the accuracy of those
interpretations is generally poor. In particular, viewers systematically underestimate the correlation
displayed in positively correlated scatterplots. This holds true for direct 
estimation [@strahan_1978; @bobko_1979; @cleveland_1982; @lane_1985; @lauer_1989;
@collyer_1990; @meyer_1992] and estimation via bisection tasks [@rensink_2017], and
is particularly pronounced for values of *r* between 0.2 and 0.6. The COVID-19 pandemic demonstrated
that lay populations, in addition to those who work with data professionally,
are now expected to be able to use and accurately interpret data
visualizations on a daily basis [@bbc_2022]. This expectation confers a responsibility on
the part of data visualization designers to design in such ways that people with
limited  statistical or graph training are able to correctly interpret
data visualizations. Doing this requires us to understand human perception,
apply this understanding to visualization design, and
test those designs in rigorous empirical work. Here, we present a fully-reproducible,
crowdsourced online experiment in which we systematically change visual features
in scatterplots to correct for a well-known bias. We combine two techniques for
correcting correlation underestimation in scatterplots and show that the
effects are stronger than what might be expected from simple additive combination. Building on this, we
present a framework for visualization design informed from the ground up by human perception.

```{r}
#| label: fig-tasks
#| fig-env: "figure*"
#| include: true
#| out-width: "100%"
#| fig-asp: 0.25
#| fig-cap: "Examples of scatterplots designed for different scatterplot-associated tasks. Both color and point shape have been used to delineate different clusters in the cluster separation plot."

# set random seed for reproducibility

set.seed(123)

# create theme so that newly generated plots match the plot example function

tasks_theme <- function() {
               theme_classic() %+replace%
               theme(axis.text = element_blank(),
               plot.margin = unit(c(0,0,0,0), "cm"),
               legend.position = "none",
               plot.title = element_text(size = 7.5, hjust = 0, vjust = 1),
               axis.title = element_blank(),
               axis.line = element_line(linewidth = 0.25),
               axis.ticks = element_blank())
               #axis.ticks.length = unit(1.375, "pt"))
}

## Create correlation plot

corr_plot <- plot_example_function(slopes,
                      0.05,
                      0,
                      "Correlation\nPerception") +
  tasks_theme() +
  geom_point(aes(alpha = 1), size = 0.5) +
  annotate(geom = "text", x = -2, y = 2, label = "r = 0.6", size = 3.5)

## Create cluster separation plot

# cluster sep data

n <- 128

cluster_sep_data <- data.frame(
  x = c(rnorm(n, mean = 0, sd = 1), rnorm(n, mean = 4, sd = 1), rnorm(n, mean = 8, sd = 1)),
  y = c(rnorm(n, mean = 0, sd = 1), rnorm(n, mean = 4, sd = 1), rnorm(n, mean = 0, sd = 1)),
  cluster = factor(rep(1:3, each = n)))

# create plot

cluster_sep_plot <- ggplot(cluster_sep_data, aes(x = x, y = y, colour = cluster)) +
  geom_point(size = 0.6, aes(shape = cluster)) +
  scale_size_identity() +
  tasks_theme() +
  labs(title = "Cluster\nSeparation")

## Outlier detection plot

# make data

n <- 127
x <- runif(n, 0, 100)
y <- 2 * x + 5 + rnorm(n, 0, 10) 
x <- c(x, 35) 
y <- c(y, 200) 

data <- data.frame(x, y)

# create plot

outlier_plot <- ggplot(data, aes(x, y)) +
  geom_point(size = 0.2) +
  scale_size_identity() +
  tasks_theme() +
  labs(title = "Scatterplot with\nObvious Outlier") +
  ylim(0,300)

## Non-linear correlation plot

# make data
  
V1 <- seq(-10, 10, length.out = 128)

V2 <- 2 * V1^2 + rnorm(length(V1), mean = 0, sd = 10)

NL_data <- data.frame(V1, V2)

# make plot

NL_corr_plot <- plot_example_function(NL_data,
                                      0.05,
                                      0,
                                      "Non-Linear\nCorrelation") +
  tasks_theme() +
  geom_point(aes(alpha = 1),
             size = 0.2)

# arrange the four task example plots

ggarrange(corr_plot,
          cluster_sep_plot,
          outlier_plot,
          NL_corr_plot,
          nrow = 1)
```

# Related Work {#sec-related-work}

## Testing Correlation Perception {#sec-testing-corr-percept}

The testing of linear correlation perception in scatterplots has a long and rich history, and has
explored a wide variety of plot types, tasks, and modeling methods. Work 
involving participants making discrimination judgements between scatterplots with different
correlations [@pollack_1960; @doherty_2007] found that performance on such tasks became better
as the objective *r* value increased. This performance
can also be modeled by deep neural networks [@yang_2023]. Extensive work throughout the 1970s to 1990s 
focused on participants' numerical estimates of correlation,
and found evidence for a systematic *underestimation* for positive *r* values besides
0 and 1. This underestimation was especially pronounced for 0.2 < *r* < 0.6
[@strahan_1978; @bobko_1979; @cleveland_1982; @lane_1985; @lauer_1989; @collyer_1990; @meyer_1992]
and is illustrated in @fig-underestimation-curve. More recent work has attempted to model participants' 
correlation estimation performance by using a combination of a bisection task, in which participants are asked to adjust a
plot until its correlation is halfway between that of two reference plots, and a
staircase method task designed to produce Just Noticeable Differences between scatterplots such
that their correlations are discriminable 75% of the time [@rensink_2010]. This work has
been extended to incorporate Bayesian data analysis [@kay_2015], which also 
identifies scatterplots as particularly suited for 
the communication of correlation [@li_2010; @harrison_2014]. The current experiment
adapts techniques from previous work [@strain_2023; @strain_2023b] and 
combines them to further push the envelope of how systematically adjusting visual
features in scatterplots can radically alter people's perceptions of correlation.
For this reason we use the same direct estimation paradigm  to collect responses;
this allows for a large number of judgements to be collected,
and is simple enough that participants need little to no training.

## Drivers of Correlation Perception {#sec-drivers}

Evidence points towards correlation perception being driven by the shape of the
underlying probability distribution represented by scatterplot points, however it should
be noted that this is very much still an open question, especially with regards to
what low level perceptual mechanisms may be at play. It is possible that
there are different contributory perceptual mechanisms operating at different levels
based on task-specific differences such as viewing time or levels of graph-training.
Increasing the *x* and *y* scales on a scatterplot such that the size of the point
cloud decreases [@cleveland_1982] is associated with an increase in viewers'
judgements of bivariate association, despite the objective *r* value remaining the same. It
was suggested in this case that viewers may have been using the area of the point
cloud to judge association. Later work found that the relationship between objective
and perceived *r* values could be described by a function that included the mean of the
geometric distances between the points and the regression line [@meyer_1997]. Investigation
of the idea that people use visual features to judge correlation provides
evidence that, among three other equally predictive features, the standard deviation of all perpendicular
distances from scatterplot points to the regression line is predictive of performance
on a correlation estimation task [@yang_2019]. Equations for both discrimination
and magnitude estimation of correlation include a quantity that is small
when *r* = 1 and increases as *r* approaches 0 [@rensink_2017]. This quantity is indifferent
to the type of visualization used (e.g. line graphs and bar charts [@harrison_2014] or
augmented stripplots [@rensink_2017]), and is functionally similar to that found
in work mentioned above [@cleveland_1982; @meyer_1997; @yang_2019]. Regarding
scatterplots, this quantity represents the average distance between data points
and the regression line, and can be thought of as closely approximating the width
of the underlying probability distribution. Findings from a convolutional neural network that
learnt visual features related to correlation perception also support 
the idea that viewers are using an aspect of the shape of the point cloud to judge correlation,
or some measure of what has been termed *dot entropy* [@yang_2023], again considered a candidate
visual proxy for correlation judgements [@rensink_2017; @rensink_2022].

Recent work investigating the use of decay functions that change point size or opacity
in scatterplots as a function of residual distance provide evidence for 
both point density and salience/perceptual weighting being drivers of correlation perception.
The use of an inverted opacity decay function [@strain_2023], such that the opacity of a point
decreased the closer it was to the regression line, resulted in significantly
lower and less accurate correlation estimates compared to uniformly full-opacity plots
with the same overall shape. This finding suggests that lower opacity in the
center of the scatterplot further biased participants towards underestimation. When point opacity
or size are reduced as a function of distance from the regression line 
[@strain_2023; @strain_2023b], viewers rate correlation as significantly higher
and are significantly more accurate, findings that support a low-level data salience
account. Our aims are to test the impact of systematically altering visual features on correlation perception 
and to provide empirically-derived tools for visualization designers to design better
visualizations informed by the use of a simple, reproducible framework.

```{r}
#| label: fig-underestimation-curve
#| include: true
#| out-width: "50%"
#| fig-asp: 1
#| fig-align: "left"
#| fig-cap: Using a function relating objective to perceived *r* value [@rensink_2017] provides a visualization of the nature of correlation underestimation reported in previous work. An identity line has been included to illustrate where viewers are most and least accurate.

my_rs <- seq(from = 0, to = 1, length.out = 100)

as_tibble(my_rs) %>%
  mutate(under_est = (log(1-0.88*my_rs)/log(1-0.88))) %>%
  ggplot() +
  geom_smooth(aes(x = my_rs, y = under_est), colour = "black") +
  geom_abline(slope = 1, intercept = 0, linetype = 2) +
  ylim(0,1) +
  xlim(0,1) +
  theme_ggdist() +
  theme(axis.title.x = ggtext::element_markdown(size = 16),
        axis.title.y = ggtext::element_markdown(size = 16),
        axis.text = element_text(size = 14)) +
  labs(x = "Objective *r* value",
       y = "Subjective *r* value")

```

## Opacity and Contrast {#sec-transparency-and-contrast}

```{r}
#| label: fig-overplotting-examples
#| include: true
#| fig-asp: 0.75
#| out-width: "50%"
#| fig-align: "left"
#| fig-cap: Adjusting point opacity to address overplotting. Contrast between the points and the background is full (alpha = 1, full opacity points, L) or low (alpha = .1, low opacity points, R). The dataset used has 40,000 points.
 
set.seed(123)

data <- data.frame(x = c(rnorm(20000, mean = -1),
                         rnorm(20000, mean = 1)),
                   y = rnorm(40000))

ggplot(data, aes(x = x, y = y)) +
  scale_alpha_identity() +
  geom_point(aes(alpha = ifelse(x > -0.1 & x < 0.1, 0,
                         ifelse(x > 0, 0.1, 1))),
             shape = 16) +
  geom_vline(xintercept = 0,
             linetype = "dashed") +
  theme_ggdist() +
  labs(x = "",
       y = "") +
  theme(axis.text = element_blank())
```

```{r}
#| label: fig-alpha-examples
#| include: true
#| out-width: "50%"
#| fig-cap: Demonstrating the effects of different alpha values on point opacity.
#| fig-height: 2
#| fig-align: "left"

x1 <- seq(0,1, length.out = 11)
y1 <- rep(1, times = 11)
df <- data.frame(x1, y1)
ggplot(aes(x1, y1), data = df) +
  geom_point(alpha = x1, size = 18, shape = 16) +
  theme(panel.background = element_blank(),
        axis.ticks = element_blank(),
        axis.text.y = element_blank(),
        axis.title = element_blank(),
        plot.title = element_text(size = 16),
        axis.text.x = element_text(size = 14)) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 11)) +
rm(x1,y1,df)
```

Changing the opacity of scatterplot points is standard practice to deal with
issues of overplotting [@matejka_2015]; scatterplots
with very large numbers of points, especially those with high degrees of overlap, suffer
from low individual-point visibility caused by high point density. Lowering
the opacity of all points using alpha blending [@few_2008] addresses this,
and makes data trends and distributions
easier to see and interpret (see @fig-overplotting-examples).
In the present study we use the **ggplot2** package [@hadley_gg2016] to create
our stimuli. This package uses an alpha parameter, or the level of linear
interpolation [@stone_2008] between foreground and background pixel values, to
set the opacity of points. As demonstrated in @fig-alpha-examples, an alpha value
of 0 or 1 results in no interpolation and rendering of either the background or foreground
pixel values respectively. Regarding the related concept of contrast, psychophysical
definitions are often based on what is being presented (e.g., gratings) or are modeled to take
into account aspects of human vision (e.g., visibility limits) [@zuffi_2007].
Our crowdsourced methodology gives us no control over the exact luminances
of our stimuli, only over the *relative* differences in luminance between scatterplot
points and backgrounds. For this reason we do not report absolute luminance nor
make any attempt to adopt a formal definition of contrast; instead we
report the alpha value used. Given that our work aims to improve correlation perception
without removing data from the scatterplot, we also incorporate point visibility testing 
(see @sec-VT). Informal point visibility piloting suggested that our smallest,
lowest opacity points had low visibility. We therefore implemented an alpha = 0.2 floor
for these points, which we judged as conferring a sufficient level of point visibility
for the range of point sizes we used.

Previous work has found that uniformly lowering the opacity of *all* 
scatterplot points relative to the background can increase the level of
underestimation error relative to full opacity, and that lowering point
opacity *as a function of distance from the regression line* is able to bias
correlation estimates upwards to partially correct for the underestimation 
observed [@strain_2023]. Evidence suggests point salience/perceptual weighting or
spatial uncertainty as drivers of this effect. Lower stimulus contrast, which for
isolated stimuli is functionally identical to lower opacity, is 
associated with lower salience [@healey_2011], can bias judgements of mean point
position [@hong_2021], increase error in positional judgements [@wehrhahn_1990],
and can result in greater uncertainty in speed perception [@champion_2017]. Due 
to mechanistic accounts of both salience/perceptual weighting and spatial uncertainty
predicting results in the same direction regarding opacity adjustments, previous work [@strain_2023] has 
been unable to determine the extent to which each is responsible for the 
observed reduction in correlation estimation error.

Micallef et al. [@micallef_2017] found that "merging, dark dots"
support correlation estimation; despite only changing point opacity in a *uniform*
manner, the sheer number of points used in that study results
in scatterplots that appear similar to those that reduce opacity as a function of residual
error [@strain_2023]. That this technique has been shown to produce more accurate
correlation estimates as compared to unadjusted scatterplots may explain why
the optimization system employed in Micallef et al. [@micallef_2017] conferred benefits
regarding correlation estimation.

## Point Size {#sec-point-size}

For discriminability reasons, scatterplots visualizing large datasets tend also to
have smaller points. Bubble charts are a subclass of scatterplot which use point
size to describe a third variable, but what experimental work there
is on the impact of point size on correlation perception is inconclusive. Some work
has found bias and variability in correlation perception to be invariant to both uniform
and irregular changes in point size [@rensink_2012; @rensink_2014], while elsewhere a strong effect of
changing point size as a function of distance from the regression line has been
reported [@strain_2023b]. Evidence points towards a salience-dominant mechanism 
in the latter case, albeit with a small effect of spatial uncertainty. There
is evidence that larger stimulus size is associated with lower levels of spatial certainty [@alais_2004],
but higher levels of salience [@healey_2011], results which are supported by evidence 
that reaction times are slower to smaller stimuli [@gramazio_2014; @osaka_1976].
The predicted effects of spatial certainty and salience on correlation perception
operate in the opposite direction to one another, which
has allowed researchers to provide evidence for the mechanistic
dominance of salience when point size decay is used. When an inverted 
size decay function is used such that smaller points are located nearer the regression
line, correlation estimation is significantly more accurate than when all points
are the same size [@strain_2023b]. In this case it was suggested that the higher
spatial uncertainty brought on by larger exterior points caused a perceptual downweighting
during correlation estimation, which is in line with work suggesting human perceptual
systems make robust use of visuo-spatial information [@strain_2023b; @warren_2002; @warren_2004].
This effect was small, meaning we do not take it into account when making
our hypotheses. Our hypotheses do, however, take into account the wider 
body of evidence regarding stimulus size, perception, and attention. Hong et al.
[@hong_2021] found that larger stimuli could significantly alter perceptions
of global scatterplot means. Participants' estimates were significantly biased
towards areas with larger points, especially when a wide range of point sizes 
were used and there was a high correlation between point sizes and position.
We formulate H3 according to these findings.

## Hypotheses

We present a single experiment based on established effects of 
adjusting point size and point opacity in scatterplots. In our study,
we combine previously independently tested point size and point
opacity decay functions in both typical orientation (opacity/size is reduced
with residual magnitude) and inverted orientation (opacity/size is increased with
residual magnitude). Throughout we refer to *congruent* and *incongruent* conditions
with respect to the combination of orientations of size and opacity decay functions.
We hypothesize that: (H1) an increased reduction in correlation estimation error will be
observed when congruent typical orientation decay functions are used; (H2) the use of 
congruent inverted orientation decay functions will produce 
the least accurate estimates of correlation; and (H3) that owing to the greater strength
of the size channel observed in previous work [@strain_2023b], there will be a 
significant difference in correlation estimates between the two incongruent orientation conditions.

# Methodology {#sec-methods}

In this section we begin by discussing our general research methods, including our approach
to crowdsourcing, our implementations of open research practices, and the modelling
methods we use to test our hypotheses. We also discuss the issues that arise
regarding the use of these methods in our particular research paradigm. We
then report on two additional pre-experimental tests that participants completed,
point visibility testing and a screen scaling task to assess dot pitch. This
section concludes by reporting on experimental procedure, participant recruitment
and characteristics, and experimental design.

## Crowdsourcing {#sec-crowdsourcing}

Much prior work on correlation perception in scatterplots has taken place
in-person, most often with graduate students with experience in statistics. While
this work is valuable, especially to perception audiences, it can struggle
to provide data that is resilient to different viewing contexts and the wide
range of levels of statistical and graph experience present in lay populations.
In addition, the ease and low-cost afforded to us by online, crowdsourced
experimental work is unmatched. Given our intended HCI and design audiences, we therefore
choose to crowdsource all participants. We acknowledge, however, that the 
technique has been affected by low quality data and skewed demographics
in the past [@chmielewski_2020; @charalambides_2021; @peer_2021]. In light of these
issues we follow published guidelines [@peer_2021] to ensure the collection of high quality 
data. Namely, we use the Prolific.co platform [@prolific] with stringent pre-screen
restrictions; participants were required to have completed at least 100 studies
using Prolific, and were required to have a Prolific score of 100, representing a 99%
approval rate. This is more strict than the 95% suggested in previous work [@peer_2021],
but has served the authors well in previous work. 

## Open Research {#sec-open-research}

This study was conducted according to the principles of open and reproducible
research [@ayris_2018]. All data and analysis code are included in a GitHub repository[^1],
which also contains instructions for building a Docker container [@merkel_2014] to fully reproduce the computational
environment used, allowing for full replications of stimuli, analyses, and the paper
itself. Ethical approval was granted by the University of Manchester's Computer Science
departmental ethics panel (Ref: 2022-14660-24397). Hypotheses
and analysis plans were pre-registered with the OSF[^2] and
there were no deviations from them.

[^1]: https://github.com/gjpstrain/size_opacity_and_scatterplots
[^2]: https://osf.io/j32sk

## Modeling {#sec-gen-modelling}

We use linear mixed effects models to model the relationships
between the combination of size and opacity decay
functions and participants' errors in correlation estimates. Models such as these
allow us to compare differences in our IV across the full range of participant 
responses, as opposed to relying purely on aggregate data and ANOVA. These models also afford us 
the ability to include random effects for participants and items, and are particularly
resilient to a range of distributional assumption violations [@schielzeth_2020].
As per our pre-registrations we preferred maximal models [@barr_2013],
including random intercepts and slopes for participants and items. The structures 
of these models were identified using the **buildmer** package
(version 2.11, [@voeten_buildmer]) in R. This package takes a maximal random
effects structure and then identifies the most complex model that converges by dropping
terms that fail to explain a significant amount of variance.

## Stimuli {#sec-scatter-gen}

45 scatterplot datasets were generated corresponding
to 45 *r* values uniformly distributed between 0.2 and 0.99, as there is evidence that
very little correlation is perceived below *r* = 0.2 [@strahan_1978; @bobko_1979; @cleveland_1982].
Using so many values for *r* allows us to paint a broader picture of people's perception
than work using fewer values. Scatterplot points were generated based on
bivariate normal distributions with standard deviations of 1 in each direction. 
In line with evidence that performance on correlation perception tasks is stable
as long as the number of points remains above 48 [@rensink_2014] and to facilitate
comparison with previous studies [@strain_2023; @strain_2023b], each scatterplot
contained 128 data points. Each scatterplot was created and presented using a 1:1 aspect ratio,
in line with findings that this facilitates optimal performance on correlation
estimation tasks [@micallef_2017]. Scatterplots were generated as 1000 $\times$ 1000 pixel .png images,
and were scaled up or down according to a participant's monitor such that they
always occupied the same proportion of the screen. Note that this results in changes
in scatterplot point sizes in proportion with the size of the plot itself. Variations 
in size such as these have been associated with changing levels of colour/opacity discriminability
[@szafir_2018; @smart_2019]. As we only allowed participants to complete
the experiment on a desktop/laptop computer, we do not consider the variations in
point size to be worthy of extended  discussion, especially as there was 
little reported effect of dot pitch (see @sec-add-analyses).
We used equation 1 to map residuals onto size and opacity values:

\begin{equation}
  point_{size/opacity} = 1 - b^{residual}
\end{equation}

When adjusting point size, we further transform values
using a scaling factor of 4 and a constant of 0.2 to ensure that the minimum point
size in the present study is both visible and consistent with that of previous work [@strain_2023; @strain_2023b].
0.25 was chosen as the value of *b*. This value was used in previous studies
that the present work builds upon, and it produces a curve approximating a 
reflection around the identity line of the underestimation curve reported in previous
studies [@rensink_2017; @strain_2023; @strain_2023b]. We acknowledge that there
may be other, more suitable values of *b*, however testing these is
outside the scope of the present work. We used this equation in typical and inverted
orientation forms applied to point size and opacity in a factorial 2 $\times$ 2 design.
Examples of the stimuli used can be seen in @fig-examples.

```{r}
#| label: fig-examples
#| fig-env: "figure*"
#| out-width: "100%"
#| fig-asp: 0.29
#| fig-align: "left"
#| include: true
#| fig-cap: Examples of the experimental stimuli used with an \textit{r} value of 0.6.

example_plots_congruent <- ggarrange(plot_example_function(slopes,
                                        (1-slopes$slope_0.25),
                                        (1-slopes$slope_0.25),
                                        "Typical Orientation Size\nTypical Orientation Opacity"),
                   plot_example_function(slopes,
                                         (1-slopes$slope_inverted),
                                         (1-slopes$slope_inverted),
                                         "Inverted Orientation Size\nInverted Orientation Opacity"), nrow = 1) +
  annotate(geom = "text",
           label = "Congruent Plots",
           x = 0.5,
           y = 0.11) +
  annotate(geom = "segment",
           x = 0.32,
           xend = 0,
           y = 0.11,
           yend = 0.11,
           size = 0.4,
           colour = "grey",
           arrow = arrow(length = unit(0.1, "cm"), angle = 90)) +
  annotate(geom = "segment",
           x = 0.68,
           xend = 1,
           y = 0.11,
           yend = 0.11,
           size = 0.4,
           colour = "grey",
           arrow = arrow(length = unit(0.1, "cm"), angle = 90))
                   
example_plots_incongruent <- ggarrange(plot_example_function(slopes,
                                         (1-slopes$slope_inverted),
                                         (1-slopes$slope_0.25),
                                         "Inverted Orientation Size\nTypical Orientation Opacity"),
                   plot_example_function(slopes,
                                         (1-slopes$slope_0.25),
                                         (1-slopes$slope_inverted),
                                         "Typical Orientation Size\nInverted Orientation Opacity"), nrow = 1)+
  annotate(geom = "text",
           label = "Incongruent Plots",
           x = 0.5,
           y = 0.11) +
  annotate(geom = "segment",
           x = 0.31,
           xend = 0,
           y = 0.11,
           yend = 0.11,
           size = 0.4,
           colour = "grey",
           arrow = arrow(length = unit(0.1, "cm"), angle = 90)) +
  annotate(geom = "segment",
           x = 0.69,
           xend = 1,
           y = 0.11,
           yend = 0.11,
           size = 0.4,
           colour = "grey",
           arrow = arrow(length = unit(0.1, "cm"), angle = 90))

ggarrange(example_plots_congruent, example_plots_incongruent, nrow = 1)
```

## Point Visibility Testing {#sec-VT}

Discussions about the size and opacity of particular scatterplot points are inherently
difficult in the context of online, crowdsourced experiments; controlling the devices
participants use to complete these kinds of experiments, beyond insisting on laptop
or desktop computers, is impossible. While this may result in a lack of consistency
in scatterplot point sizes, opacities, or contrast ratios between participants, it also provides results
that are more resilient to different viewing contexts than traditional lab-based 
experimental work. In addition to measures
implemented to ensure high quality participant data (see @sec-crowdsourcing), it is also key that we do not
inadvertently remove data from scatterplots by including points whose size or opacity
renders them invisible. We therefore included point visibility testing. Participants
viewed six scatterplots that were made up of a certain number of points between 2 and 7. These points
were of the same size and opacity as the smallest and lowest opacity points used
in the experimental items. Participants were asked to enter in a textbox how many points
were present. Participants scored an average of
`r printnum(VT$mean_VT_perc_correct)`% ($SD$ = `r printnum(VT$sd_VT_perc_correct)`%).
Despite our use of the opacity floor detailed in @sec-transparency-and-contrast,
it is clear that some of our small, low opacity points were not reliably visible, most likely
due to low contrast between the point and background, as previous work [@strain_2023b] 
found point visibility largely invariant to size. We suggest this is due to differences in
monitor specifications between participants. In reality minimum visible point size and opacity
would need to be calibrated on a per-monitor basis. We also include performance on the point
visibility test as a fixed effect in @sec-add-analyses.

## Dot Pitch {#sec-dot-pitch}

We employed a method for obtaining the dot pitch, defined as the distance in millimetres
between the centers of the pixels, of participants' monitors [@screenscale]. 
Combining this with monitor resolution information allows us to calculate the physical on-screen
size of scatterplot points. Participants were asked to hold a standard size
credit/debit/ID card (ISO/IEC 7810 ID-1) up to their screen and resize an on-screen card
until their sizes matched. We assumed a widescreen 16:9 aspect ratio and calculated
dot pitch based on these measurements. Mean dot pitch was 
`r printnum(dot_pitch$mean_dp, digits = 2)`mm ($SD$ = `r printnum(dot_pitch$sd_dp, digits = 2)`),
corresponding to a physical on-screen size of `r printnum(dot_pitch$mean_dp*13)`mm
on a 1920 $\times$ 1080 pixel monitor for the smallest points displayed on a hypothetical
`r printnum(dot_pitch$mean_width)` $\times$ `r printnum(dot_pitch$mean_height)`cm monitor. 
We include analysis with dot pitch as a fixed effect in @sec-add-analyses.

## Procedure {#sec-gen-procedure}

The experiment was built using PsychoPy [@pierce_2019] and hosted on Pavlovia.org.
Participants were only permitted to complete the experiment on a desktop or laptop
computer. Each participant was first shown the participant information sheet and provided
consent through key presses in response to consent statements. They were asked to
provide their age in a free text box, followed by their gender identity. Participants
completed the 5-item Subjective Graph Literacy test [@garcia_2016], followed by the
point visibility task described in @sec-VT and the screen scale task described in 
@sec-dot-pitch. Participants were given instructions, and were then shown examples
of scatterplots with correlations of *r* = 0.2, 0.5, 0.8, and 0.95, as piloting of
a previous experiment indicated some of the lay population may be unfamiliar
with the visual character of scatterplots. @sec-add-analyses contains further
analysis of the potential training effects of displaying these plots. Two practice trials were
given before the experiment began. Participants worked through a randomly
presented series of 180 experimental trials and were asked to
use a slider to estimate correlation to 2 decimal places. Participants were
asked to complete each trial as quickly and accurately as possible, although there
were no time limits on individual trials. Visual masks were displayed 
for 1 second in between each scatterplot presentation. Interspersed were 6 attention check trials which explicitly
asked participants to ignore the scatterplot and set the slider to 0 or 1.

## Participants {#sec-participants}

150 participants were recruited using the Prolific.co platform. Normal to 
corrected-to-normal vision and English fluency were required for participation.
In addition, participants who had completed any of our previous studies
into correlation estimation in scatterplots [@strain_2023; @strain_2023b]
were prevented from participating. Data were collected from 158 participants. 
8 failed more than 2 out of 6 attention
check questions, and, as per pre-registration stipulations, were rejected from the
study. Data from the remaining 150 participants were included in the full analysis
(`r printnum(gender$M, digits = 1)`% male, `r printnum(gender$F, digits = 1)`% female,
and `r printnum(gender$NB, digits = 1)`% non-binary).
Participants' mean age was `r printnum(age$mean, digits = 1)` (*SD* = `r printnum(age$sd, digits = 1)`).
Participants' mean graph literacy score was `r printnum(literacy$mean, digits = 1)`
(*SD* = `r printnum(literacy$sd, digits = 1)`) out of 30. The average
time taken to complete the experiment was 37 minutes (*SD* = 12.3), and is 
discussed further in @sec-add-analyses.

## Design {#sec-design}

We employed a fully repeated-measures 2 $\times$ 2 factorial design. Each participant
saw each combination of size and opacity decay function plots for a total of 180
experimental items. Participants viewed these experimental items, along with 6
attention check items, in a fully randomized order. All experimental code, materials,
and instructions are hosted on GitLab[^3].

[^3]: https://gitlab.pavlovia.org/Strain/size_and_opacity_additive_exp

# Results {#sec-results}

All analyses were conducted using R (version `r paste0(R.version$major, ".", R.version$minor)`). 
Deviation coding was used for each of the experimental factors, which allows us to
compare means of *r* estimation error for each fixed effect to the grand mean.
We used the **buildmer** and **lme4** (version 1.1-35.1 [@lme4]) packages to build a linear mixed effects model where the difference between objective and rated *r* value was predicted by the size and opacity decay
functions used. Semi-partial R^2^ was calculated using the **r2glmm** package 
(version 0.1.2 [@r2glmm]). The **emmeans** package (version 1.10.0 [@emmeans])
was used to calculate pairwise comparisons between levels of the size and opacity decay factors.

Our first two hypotheses were fully supported in this experiment. The combination of 
typical orientation size and opacity decay functions produced the most accurate estimates of
correlation, although this also resulted in a large over-correction and consequent
overestimation for many values of *r* (see @fig-diff-error-bars-plot). Our second hypothesis was also 
supported; the combination of inverted size and inverted opacity decay functions
produced the least accurate estimates of correlation. We found no support for our third
hypothesis; there was no significant difference in correlation estimates between
typical orientation size/inverted orientation opacity decay plots and 
inverted orientation size/typical orientation opacity
decay plots (Z = -2.26, *p* = .11), however we did find a significant interaction effect
that provides evidence that the combination of size and opacity decay functions
is not additive in nature.

```{r}
#| label: model
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# r estimation error modeling

model <- buildmer(difference ~ size * opacity +
                       (1 + size * opacity | participant) +
                       (1 + size * opacity | item),
                     data = size_and_opacity_exp_tidy)
```

```{r}
#| label: assign-slot

model <- model@model
```

```{r}
#| label: model-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# build comparison model with both fixed effects terms removed

model_cmpr <- comparison(model)
```

```{r}
#| label: anova0

# runs ANOVA between experimental and comparison models

anova_results(model, model_cmpr)
```


```{r}
#| label: tbl-sig
#| include: true
#| tbl-cap: Significances of fixed effects and the interaction between them. Semi-partial R^2^ for each fixed effect and the interaction term is also displayed in lieu of effect sizes.

make_sig_table(model)

```
A likelihood ratio test revealed that the model including point size
and opacity decay conditions as fixed effects explained significantly more variance
than the null ($\chi^2$(`r in_paren(model.df)`) = 
`r printnum(model.Chisq)`, *p* `r printp(model.p, add_equals = TRUE)`). There
were significant fixed effects of size decay and opacity decay, as well
as a significant interaction between the two. The experimental model has random intercepts for items
and participants, and random slopes for participants with regards to the size decay factor.
Due both to our use of a linear mixed effects model with an interaction
term, and our lack of a comparative baseline condition (i.e no size or opacity decay function used),
we do not report a traditional measure of effect size. Instead we report the amounts of variance
explained by each fixed effect term and the interaction term as semi-partial
R^2^ [@nakagawa_2013], which can be seen in @tbl-sig along with all model statistics.
Pairwise comparisons between levels of the size and opacity decay factor can be seen
in @tbl-contrasts.

```{r}
#| label: fig-emm-plot
#| fig-env: "figure*"
#| include: true
#| fig-asp: 0.4
#| out-width: "100%"
#| fig-cap: Estimated marginal means for each combination of size and opacity decay conditions, including asymptotic lower and upper confidence intervals.

as_tibble(emmeans(model, pairwise ~ size * opacity)[[1]]) %>%
  mutate("size" = recode(size,
                         "S" = "Typical Orientation Size",
                         "I" = "Inverted Orientation Size"),
          "opacity" = recode(opacity,
                         "S" = "Typical Orientation Opacity",
                         "I" = "Inverted Orientation Opacity")) %>%
  ggplot(aes(x = reorder(size:opacity, emmean), y = emmean*-1)) +
  geom_point(size = 1.75) +
  geom_pointrange(aes(ymin = asymp.LCL*-1, ymax = asymp.UCL*-1)) +
  scale_x_discrete(labels = ~ gsub(":", "\n", .x)) +
  theme_ggdist() + 
  labs(y = "Estimated Marginal Mean of Error",
       x = " Size : Opacity") +
  theme(axis.text = element_text(size = 7),
        axis.title = element_text(size = 8)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  coord_flip() +
  geom_abline(intercept = 0, slope = 0, linetype = 2) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = -.02,
                   yend = -.08,
           arrow = arrow(type = "closed", length = unit(0.1, "cm" ))) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = .02,
                   yend = .08,
           arrow = arrow(type = "closed", length = unit(0.1, "cm"))) +
  annotate(geom = "text",
           x = 4.15,
           y = -.05,
           label = "Underestimation",
           size = 3) +
  annotate(geom = "text",
           x = 4.15,
           y = .05,
           label = "Overestimation",
           size = 3) +
  annotate(geom = "text",
           x = 2.5,
           y = 0.008,
           label = "No Estimation Error",
           size = 2.5,
           angle = 270)
  
```

```{r}
#| label: tbl-contrasts
#| include: true
#| tbl-cap: Pairwise comparisons. TO = Typical Orientation, IO = Inverted Orientation. The interaction is driven by the non-additive nature of combining point size and contrast decay functions, and the only nonsignificant contrast is found when incongruent decay functions are compared.

# have a look at replacing this table with something better - it is ugly

table_df <- contrasts_extract(model) %>%
  mutate(p.value = scales::pvalue(p.value)) %>%
  rename("\\textit{p}" = "p.value",
         "Z ratio" = "Z.ratio") %>%
  mutate('Contrast' = recode(Contrast,
         "S I - I I" = "TO Size x IO Opacity | IO Size x IO Opacity",
         "S I - S S" = "TO Size x IO Opacity | TO Size x TO Opacity",
         "S I - I S" = "TO Size x IO Opacity | IO Size x TO Opacity",
         "I I - S S" = "IO Size x IO Opacity | TO Size x TO Opacity",
         "I I - I S" = "IO Size x IO Opacity | IO Size x TO Opacity",
         "S S - I S" = "TO Size x TO Opacity | IO Size x TO Opacity"))

kbl(table_df, booktabs = TRUE, digits = c(0,2,3), escape = FALSE)
```

```{r}
#| label: fig-diff-error-bars-plot
#| fig-env: "figure*"
#| include: true
#| out-width: 100%
#| fig-cap: Plots showing how participants' correlation estimation errors change as a function of the *r* value for each combination of size and opacity decay factors. Overestimation occurs above the dashed line.
#| fig-asp: 1

plot_error_bars_function(size_and_opacity_exp_tidy %>%
                        mutate(condition_abs = fct_relevel(condition_abs,
                                                          c("A", "B", "X", "Y"))),
                        "difference",
                        labels_size_opacity) +
  geom_hline(yintercept = 0, linetype = 2)
```

```{r}
#| label: additional-analyses
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

lit_model <- lmer(add.terms(formula(model), "literacy"),
                  data = size_and_opacity_exp_tidy)

anova_results(lit_model, model)

VT_model <- lmer(add.terms(formula(model), "VT_no_correct"),
                  data = size_and_opacity_exp_tidy)

anova_results(VT_model, model)

dot_pitch_model <- lmer(add.terms(formula(model), "dot_pitch"),
                        data = size_and_opacity_exp_tidy)

anova_results(dot_pitch_model, model)

training_model <- lmer(add.terms(formula(model), "half"),
                       data = size_and_opacity_exp_tidy)

anova_results(training_model, model)

timing_model <- lmer(add.terms(formula(model), "trials.thisN"),
                     data = size_and_opacity_exp_tidy)

anova_results(timing_model, model)
```

## Additional Analyses {#sec-add-analyses}

We find no effects of graph literacy ($\chi^2$(`r in_paren(lit_model.df)`) = 
`r printnum(lit_model.Chisq)`, *p* `r printp(lit_model.p, add_equals = TRUE)`),
performance on the point visibility task ($\chi^2$(`r in_paren(VT_model.df)`) = 
`r printnum(VT_model.Chisq)`, *p* `r printp(VT_model.p, add_equals = TRUE)`), or
dot pitch ($\chi^2$(`r in_paren(dot_pitch_model.df)`) = `r printnum(dot_pitch_model.Chisq)`,
*p* `r printp(dot_pitch_model.p, add_equals = TRUE)`) on participants' errors in correlation estimation.
We did find a significant effect of training ($\chi^2$(`r in_paren(training_model.df)`) = 
`r printnum(training_model.Chisq)`, *p* `r printp(training_model.p, add_equals = TRUE)`),
with participants rating correlation .01 lower during the second half. This may imply
that having more recently viewed the four training plots described in @sec-gen-procedure
increased participants' estimates of correlation. To analyse the potential variability
of participant responses in more detail, we build a model including trial number. As
the presentation order of experimental stimuli was randomized separately for each participant,
this allows us to examine responses purely as a function of *when* they were made.
We find a significant effect of trial number on participant's errors ($\chi^2$(`r in_paren(timing_model.df)`) = 
`r printnum(timing_model.Chisq)`, *p* `r printp(timing_model.p, add_equals = TRUE)`).
@fig-time-plot shows participants' unsigned mean errors in correlation estimation
against trial number. Variability in error, as represented by the ribbon in @fig-time-plot,
stabilizes quickly and remains stable for most of the experiment, only widening again
around trial number 170. We suggest that this is a result of participants knowing
that they were coming to the end of the experiment and being less vigilant as a result.
Regardless of its statistical significance, we do not consider this effect large enough to warrant
further analysis, although future work will take into account potential effects
of experiment length.

```{r}
#| label: fig-time-plot
#| fig-env: "figure*"
#| include: true
#| out-width: 100%
#| fig-asp: 0.4
#| fig-cap: Comparing mean errors in correlation estimation by trial number. Points represent unsigned mean errors for each trial number. The plotted line is the locally estimated smoothed curve, with the ribbon representing standard errors.

size_and_opacity_exp_tidy %>%
  drop_na() %>%
  group_by(trials.thisN) %>%
  summarise(sd = sd(abs(difference)), mean = mean(abs(difference))) %>%
  ggplot(aes(x = trials.thisN, y = mean)) +
  geom_point(alpha = 0.4, shape = 16) +
  geom_smooth(se = T, span = 0.5, colour = "black", size = 0.7,) +
  scale_x_continuous(breaks = seq(0,180, by = 20), limits = c(0, 180)) +
  geom_vline(xintercept = 0, linetype = 2) +
  geom_vline(xintercept = 90, linetype = 2) +
  geom_vline(xintercept = 180, linetype = 2) +
  labs(x = "Trial Number",
       y = "Unsigned Mean Error") +
  annotate("text",
           x = 45,
           y = 0.145,
           label = "1st Half") +
    annotate("text",
           x = 135,
           y = 0.145,
           label = "2nd Half") +
  theme_ggdist() +
  theme(axis.text = element_text(size = 7),
        axis.title = element_text(size = 8)) 
```

# Discussion {#sec-discussion}

The finding of a significant interaction between point size and opacity
decay provides evidence that these functions combine
in non-additive ways. In addition, we provide further confirmatory evidence of what has been found
previously. Namely, that while manipulations of both point size and opacity
have significant effects, the effect of changing point size is stronger, and that
while we can influence correlation estimates in either direction, typical
orientation manipulations are more powerful than inverted ones [@strain_2023; @strain_2023b].
As one would expect, we also see an effect of orientation congruency on the extent
to which a manipulation can bias correlation estimates; redundant 
encoding, such as that present here in congruent conditions, is known to support 
visual grouping and segmentation [@nothelfer_2017]. We now provide evidence that redundancy can be exploited
to change correlation perception. The lack of support for our third hypothesis, that there would be a
difference in correlation estimates between incongruent conditions, was surprising
given the greater strength of the size channel relative to opacity demonstrated
in previous work [@strain_2023; @strain_2023b; @hong_2021], although this may be a facet of the non-additive
interaction between size and opacity manipulations we found. Despite the lack of support for
this hypothesis, we did find that the size decay channel explained more variance (.104)
in our model than opacity decay (.087).

Taking into account the present work, which manipulates point size and opacity
together, and previous work manipulating the same visual features in isolation, we 
provide recommendations to visualization designers and researchers:

 - When *r* is between approximately 0.3 and 0.75, and the scatterplot in question
 is intended solely for the communication of correlation, designers may wish to implement
 the size decay function, as previous work [@strain_2023b] has shown it to produce the
 most accurate correlation estimates in this range.
 - Outside of this range, and with the same caveats in place, designers may wish
 to implement the opacity decay function [@strain_2023]; while its effect on correlation estimation
 is small, it does significantly increase estimation accuracy.
 - There exists a combination of size and opacity decay functions that produces
 accurate correlation estimates while maintaining the increased *r* estimation
 precision that we would expect to see with high *r* values (see @sec-precision).
 Finding this will require extensive future testing.

## Combining Manipulations {#sec-combining}

@fig-emm-plot and @fig-diff-error-bars-plot show how, on average, 
the combination of typical orientation size and opacity decay functions
results in an overestimation of *r* for the majority of values.
While this does not solve the underestimation problem, it does demonstrate
that with regards to using point size and opacity to bias viewer's estimates
of correlation in scatterplots, there would appear to be few limitations. If we
can over-correct correlation estimates, then we certainly have the ability to
correct appropriately. The issue here is not one of our *ability* to change 
people's perceptions, but of *tuning* the use of these visual features to be able
to change people's perceptions in systematic ways. We explore what further 
work would need to be done to achieve this in @sec-future-work. Combining inverted 
size and opacity decay functions also had the predicted effect in this case,
producing the lowest and least accurate estimates of correlation.
Combining inverted manipulations did not, however, significantly change the shape
of the estimation curve (see @fig-diff-error-bars-plot). In addition to interacting
non-additively, the effects we observe operate differently depending on the direction
of the change induced in perception. This finding can also explain the lack of support
found for our hypothesis stating there would be a significant difference
in *r* estimation error between the two incongruent conditions. Despite the size channel
being more powerful with regards to influencing correlation estimates, the fact
that this power depends on the direction the function is set causes incongruent
functions to act against each other in ways we would not expect. Indeed, the 
incongruent condition that used a typical orientation size decay function exhibited
lower mean error than the one using inverted orientation size decay (see @fig-diff-error-bars-plot),
however in each case opacity decay appears to have blunted the power of the
size decay function to the extent that the difference in errors is not statistically
significant.

## Estimation Precision {#sec-precision}

Much previous work is consistent regarding the finding that *r* estimation
precision increases with the objective *r* value [@rensink_2010; @rensink_2012; @rensink_2014;
@rensink_2017; @doherty_2007]. More recent work using the same size or opacity decay
functions (albeit in isolation and without an
opacity floor) [@strain_2023; @strain_2023b] found
that in some cases, precision in *r* estimation is *constant* across the range of
*r* values investigated. For example, the use of a size decay function, whether
using typical/inverted orientation non-linear functions or a linear decay
function, results in **no** change in *r* estimation precision [@strain_2023b].
When opacity is used in the same ways, only an inverted decay function **does not**
exhibit the conventional increase in precision with *r*. In the present work,
precision in *r* estimation increased whenever a typical orientation opacity
decay function was used. We suggest this is part of the moderating effect of 
the point opacity decay function on the size decay function; the visual character
of scatterplots with high *r* values that use the size decay function eliminates
the usual increase in precision we would expect, however the introduction of the 
opacity decay function normalizes this to the point where precision is restored.

## Contributions of Size and Opacity Decay

Incorporating data from previous work [@strain_2023; @strain_2023b] allows us to compare estimation
curves for size decay and opacity decay in isolation and in combination. 
@fig-est-multi-exp shows correlation estimation error curves in the present experiment,
in two previous studies that used decay functions applied solely to size or
opacity, and with no manipulations present. The "no manipulations present"
plot is averaged over conditions from previous work [@strain_2023; @strain_2023b].

```{r}
#| label: fig-est-multi-exp
#| fig-env: "figure*"
#| include: true
#| out-width: "100%"
#| fig-asp: 0.33
#| fig-cap: Plotting *r* estimation error against the objective *r* value for opacity and size decay in isolation from previous work, and for their combination in the present study. The dashed line represents 0 error in correlation estimation, and standard deviations are shown as error bars. Note that these curves have been smoothed. Overestimation occurs above the dashed line.

# dataframe containing values from previous work and the current is included
# in the data folder 

# set facet orders

facet_order <- c("opacity_manipulated", "size_manipulated", "additive_manipulation", "standard_plot")

standard_alone <- read_csv("data/all_exp.csv") %>%
    drop_na() %>%
    filter(factor == "standard_plot") %>%
    group_by(factor, my_rs) %>% 
    summarise(sd = sd(difference), mean = mean(difference)) %>%
    arrange(my_rs) %>%
    mutate(group = ceiling(row_number() / 2)) %>%
    group_by(group) %>%
    summarise(
      factor = first(factor),
      my_rs = mean(my_rs),
      sd = mean(sd),
      mean = mean(mean)
    ) %>%
  ungroup() %>%
  select(-group)

all_exp_df <- read_csv("data/all_exp.csv") %>%
    drop_na() %>%
    filter(factor != "standard_plot") %>%
    group_by(factor, my_rs) %>% 
    summarise(sd = sd(difference), mean = mean(difference)) %>%
    mutate(factor = factor(factor, levels = facet_order))

plotting_df <- rbind(all_exp_df, standard_alone)

  plotting_df %>% 
    ggplot(aes(x = my_rs, y = -1*mean)) + 
    geom_errorbar(mapping = aes(ymin = -1*mean + sd, ymax = -1*mean - sd),width = 0.01, size = 0.3) +
    theme_ggdist() +
    scale_y_continuous(breaks = seq(-0.4,1, 0.2)) +
    theme(strip.text = element_text(size = 6, margin = margin(1,0,1,0, "mm")), aspect.ratio = 1,
        axis.text = element_text(size = 7),
        axis.title.x = ggtext::element_markdown(size = 8),
        axis.title.y = ggtext::element_markdown(size = 8)) +
    facet_wrap(factor ~., ncol = 4, labeller = labeller(factor = labels_all_exp)) +
    labs(x = "Objective *r*",
         y = "Mean *r* Estimation Error") +
    geom_smooth(se = FALSE, colour = "black", size = 0.4) +
    xlim(0.2,1) +
    geom_hline(yintercept = 0, linetype = 2)

```

Using opacity decay alone significantly changes the amplitude of the estimation error curve,
while leaving its shape intact (compare opacity manipulated and no manipulation
present plots in @fig-est-multi-exp). Using size decay, however, changes both
the amplitude and shape of the underestimation error curve (compare size manipulated
and no manipulation present plots in @fig-est-multi-exp). When size and opacity decay functions are combined,
the shape appears similar to that of size alone. This is in line with
previous work establishing size as a more potent channel for the manipulation
of correlation estimates [@strain_2023b] and positional means [@hong_2021]. It would appear that the addition of
the opacity curve moderates the effect of the size curve as a function of the
objective *r* value itself, without affecting the general shape of the curve.
In the following, we briefly discuss the effects of each manipulation in isolation
and the manipulations together, before making a case for the inclusion of both
when tuning scatterplots for correlation estimation due to the complementary
benefits each confers. Throughout the course of this section it should be noted
that the analyses include data from several separate experiments [@strain_2023; @strain_2023b]. We argue that
their use of near-identical decay equations (in isolation and without the
use of an opacity floor) and experimental paradigms
render comparisons appropriate, but we acknowledge the potential for overstated conclusions.

Using an opacity decay function in isolation has a small effect on
correlation estimation. It does little to change the shape of the underestimation
curve (see @fig-underestimation-curve), but slightly biases *r* estimates up to
partially correct for the underestimation observed with normal scatterplots [@strain_2023].
Importantly, it also preserves the increase in correlation estimation precision with *r*
that we would expect to find during correlation estimation tasks. Using the size
decay function in isolation has a more dramatic effect. The shape of the estimation
curve is altered quite radically, and estimation precision does not increase
with the objective *r* value [@strain_2023b]. Size decay over-corrects at lower values of *r*,
leading to an overestimation effect, while at high values the curve begins to 
change direction, leading to a more severe underestimation. In the middle range
of *r* values, however, the size decay function in isolation performs well. One option
for tuning correlation estimation using these functions would therefore be to 
use the size decay function alone for mid-range values of *r* (0.3 to 0.75), and to use the
opacity decay function outside of this range. Used together however,
we can exploit the power of the size decay function whilst maintaining
the expected increase in precision with *r* that the opacity decay function confers.
It is clear that the simple combination we have used in the present study does
not represent an ideal tuning, as participants overestimated *r* for the majority of 
values, but this confirms that there is the scope to bias *r* estimates significantly
using the functions supplied here. Further work would be required to obtain 
precise measures of the contributions of each decay function when they are used together,
as their combination is not additive. Doing
this would allow us to tune each function according to both objective *r* value
and the tuning of the other function to facilitate more accurate correlation estimates.
We can derive new curves that describe the effect
that each manipulation and the combination of manipulations has on correlation estimates
(@fig-power-plot) by comparing them to estimates without
any manipulation present across the range of *r* values used.
We term this 'power'. The dotted line on each plot
shows the power that would be needed to correct for the standard underestimation
curve (see @fig-underestimation-curve). As we can see, size alone provides the 
closest to the requirement, and combining size and opacity decay functions 
results in gross overestimation. @fig-power-plot includes the integral of each
power curve over *r* as a measure of the total power of each curve. We also display
the difference between this integral and the integral of each required-power curve over *r*,
which shows to what extent the power we observed differs from what would be required.

```{r}
#| label: fig-power-plot
#| fig-env: "figure*"
#| include: true
#| out-width: "100%"
#| fig-cap: Power is the difference between what is observed when a decay function/combination of decay functions is used and what is observed when no manipulation is used. The dashed line represents the power that would be required to correct for the observed underestimation of correlation in scatterplots. The integral of each power curve over \textit{r} is provided, as well as the difference between this integral and the integral of each required-power curve over \textit{r}.

# dataframe containing values from previous experiments and the current is included
# in the data folder 

curves <- read_csv("data/curves_df.csv") 

facet_order <- c("opacity_power", "size_power", "additive_power")

# create necessary power curve

necessary_power <- curves %>% select(c("my_rs", "standard_curve")) %>%
  mutate(mirror = my_rs-standard_curve)

# find integrals

int_df <- left_join(curves, necessary_power, by = "my_rs")

area_opac <- round(AUC(int_df$my_rs, int_df$opacity_power) - AUC(int_df$my_rs, int_df$mirror), 3)

area_size <- round(AUC(int_df$my_rs, int_df$size_power) - AUC(int_df$my_rs, int_df$mirror), 3)

area_combined <- round(AUC(int_df$my_rs, int_df$additive_power) - AUC(int_df$my_rs, int_df$mirror), 3)

int_opac <- round(AUC(int_df$my_rs, int_df$opacity_power), 3)

int_size <- round(AUC(int_df$my_rs, int_df$size_power), 3)

int_combined <- round(AUC(int_df$my_rs, int_df$additive_power), 3)

# create area and int labels for facetted plots

area_labels <- tibble(
  label = c(substitute(expression(integral(italic(f)(italic(x)) - italic(g)(italic(x))*italic(dx), 0.2, 1) == a), list(a = area_opac)),
            substitute(expression(integral(italic(f)(italic(x)) - italic(g)(italic(x))*italic(dx), 0.2, 1) == a), list(a = area_size)),
            substitute(expression(integral(italic(f)(italic(x)) - italic(g)(italic(x))*italic(dx), 0.2, 1) == a), list(a = area_combined))),
  factor = c("opacity_power", "size_power", "additive_power"),
)

int_labels <- tibble(
  label = c(substitute(expression(integral(italic(f)(italic(x))*italic(dx), 0.2, 1) == a), list(a = int_opac)),
            substitute(expression(integral(italic(f)(italic(x))*italic(dx), 0.2, 1) == a), list(a = int_size)),
            substitute(expression(integral(italic(f)(italic(x))*italic(dx), 0.2, 1) == a), list(a = int_combined))),
  factor = c("opacity_power", "size_power", "additive_power"),
)

# create plot

 curves %>% 
    drop_na() %>%
    select(c("opacity_power",
             "size_power",
             "additive_power",
             "my_rs")) %>%
    pivot_longer(cols = c("opacity_power",
                          "size_power",
                          "additive_power"),
                 names_to = "factor", values_to = "power") %>%
    mutate(factor = factor(factor, levels = facet_order)) %>%
    group_by(factor, my_rs) %>% 
    summarise(sd = sd(power), mean = mean(power)) %>% 
    ggplot(aes(x = my_rs, y = mean)) + 
    theme_ggdist() +
    scale_y_continuous(breaks = seq(-0.4,1, 0.2)) +
    theme(strip.text = element_text(size = 6, margin = margin(1,0,1,0, "mm")), aspect.ratio = 1,
          axis.text = element_text(size = 7),
          axis.title.x = ggtext::element_markdown(size = 8),
          axis.title.y = ggtext::element_markdown(size = 8)) +
    facet_wrap(factor ~., ncol = 5, labeller = labeller(factor = labels_power)) +
    labs(x = "Objective *r*",
         y = "Power") +
    geom_smooth(se = FALSE, colour = "black", size = 0.4) +
    geom_smooth(data = necessary_power,
                aes(x = my_rs, y = mirror),
                se = F,
                linetype = 3,
                colour = "black",
                size = 0.4) +
    xlim(0.2,1) +
    ylim(-0.1,0.3) +
    geom_text(data = area_labels,
              parse = T,
              mapping = aes(x = -Inf, y = -Inf, label = gsub("^.(.*).$", "\\1", str_replace(label, "expression", ""))),
              hjust = -0.4,
              vjust = -0.2,
              size = 2.5) +
       geom_text(data = int_labels,
              parse = T,
              mapping = aes(x = -Inf, y = -Inf, label = gsub("^.(.*).$", "\\1", str_replace(label, "expression", ""))),
              hjust = -0.6,
              vjust = -1.6,
              size = 2.5)

```

## Mechanisms {#sec-mechs}

Previous work has made the case for opacity and size decay acting primarily 
through salience/perceptual weighting regarding correlation estimation, with the caveat that spatial certainty
also plays a small part in the mechanism behind size decay [@strain_2023b].
Our results are supportive of this notion, with our highest and lowest estimates 
being observed in the congruent typical and congruent inverted conditions respectively. These
findings also support dot density [@yang_2023] and feature-based attentional bias accounts
[@hong_2021; @sun_2016]. As all of these mechanisms would be expected to operate
in the same direction, making conclusions about the relative contributions of
each is difficult. The body of evidence generally points to a high-level probability
distribution account [@rensink_2017; @rensink_2022]. On a lower level, numerous candidate
mechanisms exist, which are mostly expected to act in the same direction. Previous
work concluded that spatial certainty [@strain_2023b] may play a small role with 
regards to the effects of size decay on correlation perception; our results
neither confirm nor refute this, but instead provide further evidence for
salience/perceptual weighting/dot density changing participants' perceptions
of the width of a probability distribution to affect correlation estimates.
Hong et al. [@hong_2021] found that the inclusion of larger
or more opaque scatterplot points was able to bias estimates of positional means, but 
that the relative contributions (weights) of these visual features with regards
to perception change as a function of the ranges of sizes/opacities used. It is clear
from this evidence and the present study that the perceptual weights of point size
and opacity are not the same.

## Limitations {#sec-limitations}

Firstly, our participants' performance on the point visibility task was poor, with
an average score of `r printnum(VT$mean_VT_perc_correct)`%. 
It is clear from these results that for many participants, the smallest and lowest
opacity points we used were simply not visible, although it would seem that this
low visibility had no significant effect on correlation estimates. Regardless,
for many of our participants it will have appeared that we were removing data, which
violates our intended aims. Addressing this would require a by-participant calibration
of point size and point opacity, which is beyond the scope of our current methodology,
as it would require stimuli to be fully re-generated for each participant. We aim to
implement this in the future, although it would require the use of a platform
such that experimental stimuli could be regenerated per a calibration task completed
by the participant. We cannot say precisely
what proportions of the observed effect in the typical orientation congruent condition
were due to size or opacity decay. We can conclude that these effects are not linearly
additive, but must suggest further work to define precisely each of their contributions.
While our provision of dot pitch measurements is a step in the right direction,
the present methodology leaves us unable to comment on the variability in participants'
viewing contexts. We argue that our large sample size and recruitment from lay 
populations renders our conclusions resilient to changes in viewing contexts,
but further, in-person, experimentation is planned to confirm this.

Due to extensive previous testing of no-adjustment, size-only, and opacity-only
manipulation scatterplots [@strain_2023; @strain_2023b], we chose not to include
these as conditions in the present work. We did not consider the increased
costs and experimental length worth the inclusion of three extra
conditions, although we acknowledge that these would assist us in making
claims about additivity and the relative contributions of point size and opacity decay.

Channels such as point size, colour, opacity, and shape have been used in past
work to encode variables beyond the standard two typically used in scatterplots [@hong_2021; @smart_2019].
While we focus purely on correlation estimation in the present work, we acknowledge
that the use of our techniques is likely to lead to incorrect interpretations, especially
when scatterplots are designed with other tasks in mind. Given evidence that size,
shape, and colour are not entirely separable scatterplot 
features [@smart_2019], if viewers assume that variations in point size/opacity correspond to additional
encoded variables, confounds in interpretation may be introduced. As our contribution in this paper
is providing evidence that changing certain visual features in systematic ways can
alter viewers' estimates of correlation, we do not consider this to be problematic.
If plots such as the ones we have presented were to appear in the wild, however,
it would be necessary to clarify that they were designed to aid in the
rapid and intuitive interpretation of correlation (and *only* this). Irrespective
of the potential for misinterpretation, we provide strong baseline evidence for 
a perceptual effect of changing point size and opacity in scatterplots that may
be expanded on and further exploited in future work.


While we put forward salience as the most likely driver of the effects on correlation
estimation we observe when using point size/opacity manipulations, the data we have
gathered do not allow us to comment on the reasons behind the differences in 
the shapes of the estimation error curves (see @fig-est-multi-exp). The context
that the manipulation is used in (i.e. the *r* value of the scatterplot) interacts
with point size and opacity manipulations in complex ways. To understand how point size/opacity
manipulations operate in more detail, future work may wish to choose fewer *r* values
and instead vary the type of decay equation or range of sizes/opacities, but
this is beyond the scope of the current work.

## Future Work {#sec-future-work}

There is evidence that viewers overestimate correlation in negatively correlated
scatterplots [@sher_2017]. Findings that correlation perception in negatively
correlated scatterplots functions symmetrically to that of positively correlated
scatterplots [@harrison_2014] suggest that the techniques we have implemented in the current
paper may be used (in a symmetrical manner) to address the overestimation bias.
We found evidence that the influence of size and opacity decay functions changes according to the direction
they are operating in, meaning experimental work with negatively correlated
scatterplots would be required, and results may differ significantly from
findings related to the underestimation of correlation in positively
correlated scatterplots. For size and opacity decay in the present work we used equation 1. 
Given our finding that the combination is non-additive, there are a multitude of adjustable parameters 
for each decay function that require rigorous testing
in order to produce concrete values of the contributions of each. The value of 
*b* is one such parameter. We used the same value of *b* (0.25) as previous
work [@strain_2023; @strain_2023b]. Changing this value can increase or decrease
the severity of the decay function in question. Additionally, we used a constant and a scaling factor
with the size decay manipulation to ensure our points were visible. These values
could also be changed. Aside from changing aspects of equation 1, there are other equations
that could be used, including ones that take into account objective *r* value to
change the values used to set point size or opacity. Future experimental work may
use the major axis through the probability ellipse instead of the regression line
as a baseline to change point sizes and opacities; evidence that people often report
the major axis when asked to visually estimate the regression line [@collyer_1990]
suggests that this may produce a different pattern of results from those seen here.
If changes in dot density are driving changes in correlation estimates, the congruent
conditions here are an example of redundant encoding. Future work may explore using different
channels to redundantly encode dot density, such as marker shape, orientation, or color.
The present study opens the door for this future work, as it provides the necessary additional data to 
previous experiments using only size [@strain_2023b] or opacity [@strain_2023] decay functions.
Further testing of these manipulations in isolation and combination using
different decay function parameters will allow researchers to build a more complete
picture of how these visual features impact correlation estimation, and how we
can exploit them to correct for well-known biases.

Through this work we also provide an example of an experimental framework
that we argue should be employed to test a wider range of data visualizations,
statistical summaries, and task types. Our framework is fully open source, and
can be easily adapted for other charts and modalities. Doing so will further the 
cause of empirically-informed data visualization design.

# References {-}

 